{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns       \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_segmentation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-f8fc1d665bb5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'customer_segmentation.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 610\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    611\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    460\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1048\u001B[0m             )\n\u001B[1;32m   1049\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1050\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1051\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1052\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1866\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1867\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1868\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1869\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"storage_options\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"encoding\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"memory_map\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1361\u001B[0m         \"\"\"\n\u001B[0;32m-> 1362\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1364\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'customer_segmentation.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('customer_segmentation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['order_purchase_timestamp', 'order_approved_at', 'order_approved_at', 'order_delivered_carrier_date',\n",
    "            'order_delivered_customer_date', 'order_estimated_delivery_date', 'shipping_limit_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING THE VARIABLES INTO DATE BY USING PANDAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dates:\n",
    "    data[i] = pd.to_datetime(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CAN ALSO DISTINCT DATE FROM TIME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_date'] = [d.date() for d in data['order_purchase_timestamp']]\n",
    "data['order_time'] = [d.time() for d in data['order_purchase_timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()  #Dataset visualization with all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns  #Columns of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()  #The output of number of unique values is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['order_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seller_city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  #The sum of th enull values for each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CAN REMOVE THE DUPLICATES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()\n",
    "data.drop_duplicates(keep='first', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)  #resetting the index of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLIERS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data.reset_index().plot(kind='scatter', x='index',\n",
    "                      y='payment_installments', c='brown')\n",
    "\n",
    "plt.figure() \n",
    "data.reset_index().plot(kind='scatter', x='index', y='payment_value', c='gray')\n",
    "\n",
    "plt.figure()\n",
    "data.reset_index().plot(kind='scatter', x='index', y='price', c='orange')\n",
    "\n",
    "plt.figure()\n",
    "data.reset_index().plot(kind='scatter', x='index', y='freight_value', c='purple')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze the 'payment_value' variable because in our mind is one of the most interesting and important variable to study for a big store store like the one in our case. Since it seems having some values that exceed the average, we can suppose that there are some customers who have paid an amount of money which is relatively higher than the other and this can affect our analysis in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pv = data[\"payment_value\"].mean()  #result mean = 195\n",
    "std_pv = data[\"payment_value\"].std()    #result Standard Deviation = 295.5\n",
    "print(mean_pv)\n",
    "print(std_pv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the scatterplot of the outliers, we can notice that there are few payments above (around) 3500$, so we want as an output all the payments above that amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier1 = data[data['payment_value'] > 3500]\n",
    "print('\\nOutlier dataframe:\\n', outlier1)\n",
    "\n",
    "#the output will be 5 payments, each one with all its attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now delete those outliers as they're not relevant for out analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data.payment_value > 3500].index)\n",
    "data = data.reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELATIONSHOP ANALYSIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRELATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print an heatmap matrix in order to visualize clearly the correlation between each couple of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation, xticklabels = correlation.columns, yticklabels = correlation.columns, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)  #This method allows us allows us to plot pairwise relationships between variables within a large dataset dataset like ours, so that we can visualize everything in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x = 'order_item_id', y = 'payment_type', hue = 'customer_state', data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['price'])  #Seaborn Distplot represents the overall distribution of continuous data variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHERE DO MOST CUSTOMERS COME FROM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = data.groupby('customer_state').count()['customer_id'].reset_index()\n",
    "city_data = data.groupby('customer_city').count()['customer_id'].reset_index()\n",
    "\n",
    "plt.figure(figsize = (25,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "base_color = sns.color_palette()[0]\n",
    "\n",
    "sns.barplot(data = state_data.sort_values('customer_id', ascending = False), x = 'customer_state', y = 'customer_id', color = base_color)\n",
    "plt.title('Number of Customers Per State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "plt.subplot(122)\n",
    "base_color = sns.color_palette()[0]\n",
    "\n",
    "sns.barplot(data = city_data.sort_values('customer_id', ascending = False).nlargest(10,'customer_id'), x = 'customer_id', y = 'customer_city', color = base_color)\n",
    "plt.title('Cities with the Most Customers')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Customers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the state with the higher amount of customers is SP, while the city is Sao Paulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT ARE THE MOST FREQUENT ITEMS BOUGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seller_p = data['product_category_name_english'].value_counts(\n",
    ").reset_index().nlargest(10, 'product_category_name_english')\n",
    "worst_seller_p = data['product_category_name_english'].value_counts(\n",
    ").reset_index().nsmallest(10, 'product_category_name_english')\n",
    "print(best_seller_p)\n",
    "print(worst_seller_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOP 10 items bought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "sns.barplot(data = best_seller_p, x = 'product_category_name_english',\n",
    "            y = 'index')\n",
    "plt.title('Top 10 product Categories Ordered', fontweight='bold')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOWEST 10 items bought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(212)\n",
    "sns.barplot(data = worst_seller_p, x = 'product_category_name_english',\n",
    "            y = 'index')\n",
    "plt.title('Lowest 10 Product Categories Ordered', fontweight = 'bold')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.ylabel('Product Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bed and bath products are the top products ordered followed by beauty products and housewares.\n",
    "\n",
    "Musical products have the lowest amount of products ordered, followed by flowers and children clothes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHICH ARE THE MOST COMMON PAYMENT TYPES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_types = data['payment_type'].value_counts().reset_index() #count the number of different types of payment\n",
    "print(payment_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHICH ARE THE NUMBER OF ORDERS PER PAYMENT TYPE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "sns.barplot(data = payment_types, x = 'index', y = 'payment_type')\n",
    "plt.title('Orders by Payment type')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Orders');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHICH IS THE NUMBER OF ORDERS WITH NUMBER OF PAYMENT INSTALLMENTS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(122)\n",
    "sns.barplot(data = data['payment_installments'].value_counts().reset_index(), x = 'index', y = 'payment_installments')\n",
    "plt.title('Count of Orders With Number of Payment Installments')\n",
    "plt.xlabel('Number of Payment Installments')\n",
    "plt.ylabel('Count of Orders');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFM ANALYSIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with RECENCY, which is the time since a customer's last purchase. It is calculated by subtracting the customer's last shopping date from each shopping timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recency'] = print((max(data['order_purchase_timestamp']) - data['order_purchase_timestamp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only want to know the days since a customer's last purchase, we could have used directly the variable 'order_date' that we initialized at the beginning of the project, but we noticed that it seems to round up the days by considering if the time of the purchase is close to the next day or not, so we are leaving both methods for sake of completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recency'] = (max(data['order_date']) - data['order_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, for the customers who made more than one purchase we will only consider the most recent purchase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recency'] = data.groupby(['customer_unique_id'], as_index=False)['recency'].transform('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step we will calculate mean, standard deviation, minimum value and maximum value for recency (we will do the same with the other two components of RFM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recency'] = data['recency'].dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean: ', data['recency'].mean())\n",
    "print('std: ', data['recency'].std())\n",
    "print('max: ', data['recency'].max())\n",
    "print('min: ', data['recency'].min())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FREQUENCY: This is the total number of purchases of the customer. In a different way, it gives the frequency of purchases made by the customer. We just count the number of purchases per customer, obviously considering each customer's unique id as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frequency'] = data.groupby(['customer_unique_id'], as_index=False)['order_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean: ', data['frequency'].mean())\n",
    "print('std: ', data['frequency'].std())\n",
    "print('max: ', data['frequency'].max())\n",
    "print('min: ', data['frequency'].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONETARY: It is the total amount of money spent by the customer. It can be calculted by summing all the 'payment_values':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['monetary'] = data.groupby(['customer_unique_id'], as_index=False)['payment_value'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean: ', data['monetary'].mean())\n",
    "print('std: ', data['monetary'].std())\n",
    "print('max: ', data['monetary'].max())\n",
    "print('min: ', data['monetary'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the computation of Frequency and Monetary, the columns of 'order_id' and 'payment_value' are replaced by the result of the computation (sum and count), even if in the putput they might not have the name changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to create a first segmentation of the customers based on the RFM. To do this we are going to convert ther RFM scores to a single variable from 1 to 5. We are going to choose the cluster a customer will be assigned to based on the values of mean, std, min and max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recency_score'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.index:\n",
    "    if data['recency'][i] <= 20:\n",
    "        data['recency_score'][i] = 5\n",
    "    elif (data['recency'][i] > 20) and (data['recency'][i] <= 65): \n",
    "        data['recency_score'][i] = 4\n",
    "    elif (data['recency'][i] > 65) and (data['recency'][i] <= 120):\n",
    "        data['recency_score'][i] = 3\n",
    "    elif (data['recency'][i] > 120) and (data['recency'][i] <= 200):\n",
    "        data['recency_score'][i] = 2\n",
    "    elif (data['recency'][i] > 200): \n",
    "        data['recency_score'][i] = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, talking about recency, we know that the mean is 72 days, so we thought that giving 4 to those customers that haven't bought for 20-65 days was the best option. For the other intervals of time, we kept it quite large considering that the strandard deviation for this score is 42 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frequency_score'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.index:\n",
    "    if data['frequency'][i] >= 9:\n",
    "        data['frequency_score'][i] = 5\n",
    "    elif (data['frequency'][i] > 7) and (data['frequency'][i] < 9):\n",
    "        data['frequency_score'][i] = 4\n",
    "    elif (data['frequency'][i] >= 5) and (data['frequency'][i] < 7):\n",
    "        data['frequency_score'][i] = 3\n",
    "    elif (data['frequency'][i] >= 2) and (data['frequency'][i] < 5):\n",
    "        data['frequency_score'][i] = 2\n",
    "    elif (data['frequency'][i] == 1):\n",
    "        data['frequency_score'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With frequency we have been a little bit more 'generous' because, considering that the maximum amount of items bought is 13 and that, on average, the items bought are between 1 anmd 2, we think that a customer who bought more than 9 items can be considered a customer of level 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONETARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['monetary_score'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-38e36a3bc303>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'monetary'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'monetary_score'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'monetary'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m250\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'monetary'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'monetary_score'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for i in data.index:\n",
    "    if data['monetary'][i] > 500:\n",
    "        data['monetary_score'][i] = 5\n",
    "    elif (data['monetary'][i] > 250) and (data['monetary'][i] <= 500):\n",
    "        data['monetary_score'][i] = 4\n",
    "    elif (data['monetary'][i] > 150) and (data['monetary'][i] <= 250):\n",
    "        data['monetary_score'][i] = 3\n",
    "    elif (data['monetary'][i] > 100) and (data['monetary'][i] <= 150):\n",
    "        data['monetary_score'][i] = 2\n",
    "    elif (data['monetary'][i] <= 100):\n",
    "        data['monetary_score'][i] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create columns in order to group all the scores. Then, we extracted from the original dataset only the columns of interest so that we can have a better visualization, keeping only the customer id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recency:\n",
    "data[\"recency_score\"] = pd.qcut(data['recency'], 5, labels=[5, 4, 3, 2, 1])\n",
    "#Frequency:\n",
    "data[\"frequency_score\"] = pd.qcut(data[\"frequency\"].rank(method=\"first\"), 5, labels=[1, 2, 3, 4, 5])\n",
    "#Monetary:\n",
    "data[\"monetary_score\"]= pd.qcut(data[\"monetary\"],5,labels=[1,2,3,4,5])\n",
    "\n",
    "rfm_data = data[[ 'recency', 'frequency', 'monetary', 'recency_score', 'frequency_score', 'monetary_score']]\n",
    "\n",
    "rfm_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = rfm_data[['recency', 'frequency', 'monetary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "rfm_std = scaler.fit_transform(rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for K-means algorithm and performance measurments\n",
    "\n",
    "# Using elbow method to find out optimal k value with WCSS score\n",
    "# Creating empty list to insert values\n",
    "WCSS = []\n",
    "\n",
    "# Using a for function to fill the list with the WCSS scores\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(rfm_std)\n",
    "    WCSS.append(kmeans.inertia_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "# Use fit_predict to cluster the dataset\n",
    "km_clusters = kmeans.fit_predict(rfm_std)\n",
    "\n",
    "rfm_std_cl = pd.DataFrame(rfm_std, columns=rfm.columns, index=rfm.index)\n",
    "rfm_std_cl['cluster'] = km_clusters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Elbow method graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Elbow method graph\n",
    "plt.figure()\n",
    "plt.plot(range(1, 11), WCSS, marker='o', label='line with marker')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "# Use fit_predict to cluster the dataset\n",
    "km_clusters = kmeans.fit_predict(rfm_std)\n",
    "\n",
    "rfm_std_cl = pd.DataFrame(rfm_std, columns=rfm.columns, index=rfm.index)\n",
    "rfm_std_cl['cluster'] = km_clusters\n",
    "\n",
    "# Visualising the clusters (4 clusters according to the Elbow method)\n",
    "plt.scatter(rfm_std[km_clusters == 0, 0], rfm_std[km_clusters ==\n",
    "            0, 1], s=30, c='orange', label='Cluster 1')\n",
    "plt.scatter(rfm_std[km_clusters == 1, 0], rfm_std[km_clusters ==\n",
    "            1, 1], s=30, c='yellow', label='Cluster 2')\n",
    "plt.scatter(rfm_std[km_clusters == 2, 0], rfm_std[km_clusters ==\n",
    "            2, 1], s=30, c='brown', label='Cluster 3')\n",
    "plt.scatter(rfm_std[km_clusters == 3, 0], rfm_std[km_clusters ==\n",
    "            3, 1], s=30, c='purple', label='Cluster 4')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[\n",
    "            :, 1], s=70, c='black', label='Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating cluster validation metrics (silhouette score, calinski harabasz score, davies bouldin score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_s = silhouette_score(rfm_std, kmeans.labels_, metric='euclidean')\n",
    "kmeans_c = calinski_harabasz_score(rfm_std, kmeans.labels_)\n",
    "kmeans_d = davies_bouldin_score(rfm_std, km_clusters)\n",
    "print('Silhouette Score for K-Means:', kmeans_s)\n",
    "print('Calinski Harabasz Score for K-Means:', kmeans_c)\n",
    "print('Davies Bouldin Score for K-Means:', kmeans_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the dendogram\n",
    "plt.figure()\n",
    "dendrogram = sch.dendrogram(sch.linkage(rfm_std, method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Hierarchical Clustering to the dataset\n",
    "hc = AgglomerativeClustering(\n",
    "    n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "hc_clusters = hc.fit_predict(rfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters (4 clusters according to the Dendogram)\n",
    "plt.figure()\n",
    "plt.scatter(rfm_std[hc_clusters == 0, 0], rfm_std[hc_clusters == 0, 1], s = 30, c = 'orange', label = 'Cluster 1')\n",
    "plt.scatter(rfm_std[hc_clusters == 1, 0], rfm_std[hc_clusters == 1, 1], s = 30, c = 'yellow', label = 'Cluster 2')\n",
    "plt.scatter(rfm_std[hc_clusters == 2, 0], rfm_std[hc_clusters == 2, 1], s = 30, c = 'brown', label = 'Cluster 3')\n",
    "plt.scatter(rfm_std[hc_clusters == 3, 0], rfm_std[hc_clusters == 3, 1], s = 30, c = 'red', label = 'Cluster 4')\n",
    "plt.title('Clusters of customers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cluster validation metrics\n",
    "hierarchical_s = silhouette_score(\n",
    "    rfm_std, hc.labels_, metric='euclidean')\n",
    "hierarchical_c = calinski_harabasz_score(rfm_std, hc.labels_,)\n",
    "hierarchical_d = davies_bouldin_score(rfm_std, hc_clusters)\n",
    "print('Silhouette Score for Hierarchical:', hierarchical_s)\n",
    "print('Calinski Harabasz Score for Hierarchical:', hierarchical_c)\n",
    "print('Davies Bouldin Score for Hierarchical:', hierarchical_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1,11):\n",
    "   model = KMeans(n_clusters = i, init = \"k-means++\")\n",
    "   model.fit(rfm_std)\n",
    "   wcss.append(model.inertia_)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,11), wcss, marker='o', label='line with marker')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(3)\n",
    "data2 = pca.fit_transform(rfm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "lbls = [str(x) for x in range(1,len(var)+1)]\n",
    "plt.bar(x=range(1,len(var)+1), height = var, tick_label = lbls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters = 4, init = \"k-means++\")\n",
    "label = model.fit_predict(data2)\n",
    "plt.figure(figsize=(10,10))\n",
    "uniq = np.unique(label)\n",
    "for i in uniq:\n",
    "   plt.scatter(data2[label == i , 0] , data2[label == i , 1] , label = i)   \n",
    "plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[\n",
    "            :, 1], s=70, c='black', label='Centroids')\n",
    "#This is done to find the centroid for each clusters.\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_s = silhouette_score(data2, model.labels_, metric='euclidean')\n",
    "pca_c = calinski_harabasz_score(data2, model.labels_)\n",
    "pca_d = davies_bouldin_score(data2, label)\n",
    "print('Silhouette Score for PCA:', pca_s)\n",
    "print('Calinski Harabasz Score for PCA:', pca_c)\n",
    "print('Davies Bouldin Score for PCA:', pca_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
